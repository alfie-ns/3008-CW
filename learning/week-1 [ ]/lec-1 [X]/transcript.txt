
So what we are going to do in big data is the idea is we're going to introduce you to practices that are still emerging for large scale databases and large scale data sets.
You will be working with both structured and what we call unstructured or semi structured data.
And we'll be looking at how we store this and how we analyze it and we're going to be looking at basically graphical databases.
There are other non relational databases out there available, but we'll be focusing on the more mathematical based graphical databases.
So graph databases, they look like graphs, nodes, edges, connecting everything.
The focus of graphical databases is on the relationship between data rather than the actual data itself, allowing us to explore those relationships and hopefully find some causations and correlations between them.
Although correlation and causation aren't always equal.
And we're looking at one of the key systems that they get used for is Recommender system.
So anybody who has watched Netflix, YouTube, TikTok, all these algorithms that keep pumping that content your way, Recommender system.
So we'll be studying how we make them and you'll be able to make your own by the end of this.
And then big data, so we're working with large data sets.
The data set you've got for your coursework are 4 point something and 6 point gigabytes respectively.
So they are annual UK population survey data.
One has over 500 variables with 300,000 odd observations.
One has just over 450 variables with again order of hundred thousands observations in them.
And part of your coursework will be reprocessing, analyzing and making recommendations from that data set.
We will be covering document classification, looking at topic mining, realization and then prediction because that's what everybody wants to know.
These large data sets is what can we tell about the future from them?
As well as making pretty pictures, how are you going to be assessed?
So anybody coming from mathematics?
No exams, 100% coursework split across two elements, each released to the DLE.
I believe I've got it timed to be revealed next week.
So we've got a week's introduction and then coursework will be released.
Formative feedback is available before the session for submissions.
Even so we do have these sessions.
I don't want to be speaking for the full two hours.
So you'll notice that we're in the labs, both of them.
The idea is actually that it's a bit of I do and you do at the same time.
So there is no set lab session.
It's a mixture of lab and tutorial.
Within each of those sessions I aim to only be speaking at you.
I don't want to speak at you, but you know what I mean.
For basically the first maybe hour or so.
The second half is given over to practical examples and tutorial.
The 11th week of the semester there are 10 weeks of content.
Everything you need for the coursework will be delivered by week nine.
That's before the Easter holiday.
See what I did there?
And then week 10 is about interactive visualization.
So it's bit of an extra where big data is going in the future, making people able to manipulate data sets themselves and visualize it.
Then week 11 there is no content in week 11.
This is done on purpose because it's a the week before the showcase for those of you in computing.
So you're probably all going to be panicking about your final year project.
Is it going to be running next week?
Am I having an existential crisis?
All the above.
So week 11, no talk material.
But the labs are here so they are an opportunity for you to get that formative feedback on the report before you submit it or have an existential crisis or both.
Tissue is always on hand.
Please remember the EC policy is there.
Don't submit something that's half baked because you feel you've got to get it in.
If something does come up, please make use of this policy as what it is there for.
Details.
Two parts.
The exercise part is going to be on non relational databases.
Okay.
So the AI usage for that one is zero.
It is solo work.
The idea behind that is because this is the first time you're going to see graphical databases.
So I want to ensure that you can code it properly.
That way when you do go on to using AI to help support you, you know when it's hallucinating and tell you something that is complete and utter garbage.
Okay, so that one is solo work.
The report is assisted so you can use AI for structuring, help your research.
You can also help it to do your coding.
I do not want to see your code in the report.
Okay?
That's quite unusual for a computing module.
The idea is big data analytics.
I want to see your reasoning for your analytics.
I don't care how you produced your plot, I don't care what programming language you built your regression model in.
It's the interpretation that is key for this module.
So there is no requirement to see your code I don't want a GitHub link.
Don't need to see your code.
I will give you a lot of the code that you can use in your report.
Okay?
The emphasis is analytics.
What are you understanding from it?
Details for each one is under the Assessment tab on the dle.
There'll also be two folders.
Please don't get them confused.
You've got one set of data for your non relational database.
You have a second set of data for the report.
They are in separate folders.
One called Data for Set Exercises and the other one called Data for Report.
Please don't get them confused, otherwise the task will make no sense whatsoever.
Okay, the data you've got for your non relational database is based on Eurovision.
The data you've got for your report is the UK annual survey.
Okay, so asking the government asking you to assess the impact of COVID on economics is not going to make any sense when you're looking the fact that abba won the 1974 year of his Own contest.
So please don't get them confused.
As hilarious as that would be for me to read a report that got those two data sets confused.
For your own sanity and Mark, please don't so deadlines 3rd of March for the 70% 5th of May, 5th of May 4th of 70% both 3pm so if you are away in other countries please UK time.
If you have any trouble try to upload it before the 3pm deadline because it is not a valid EC.
So please don't be leaving it to the last minute.
Don't just just more stress that quite frankly at this point in your degree we don't need.
We're on the downhill slope.
We can see the end.
Let's make it smooth and have a bit of fun.
So support text There are a number of them.
There is an even bigger reading list available through the DLE here.
Just a couple of new ones and I tend to go to Top two are for the graphical databases.
The bottom two are more of the big data analytic side of it.
Please don't read too much into the learning outcomes for this module.
We were going to update them for this year but then it got put back so we didn't update them.
Because I am not spending four hours talking about databases.
I like databases.
I don't like databases that much.
Plus big data analytics is a lot more than just databases.
There is also some pretty good LinkedIn courses.
On the specific one we are going to be using neote4j there's a nice little reference card with the key Commands that you will be using quite a lot.
There is also a fantastically organized GitHub repository by myself.
Any code that we do during the lab and lecture sessions will be uploaded to that GitHub in the PDF.
They are all hyperlinked up so you can just click from the PDF and it will take you straight to the amazing GitHub repo that is for 3008 loads of code all up there for you to use for report and set exercises.
So today, now we finally got here.
What is big data?
What do you mean by big data?
What the properties that it has.
We're not going to import large data sets into Neo4J.
That's for Thursday, as is indexing, so why have I left those there?
But what we will be able to do is explain what we mean by big data.
Five Bs of big data, correct at time of recording and then we're going looking at how we create records in Neo 4j.
So that simplistic first step into the graphical database.
So, big data, what do we mean by big data?
So typically, think Excel, what could we put in Excel?
And then it gets to the point where you know where it has that warning and it goes, if you save this now, you're going to lose all your data.
That's central.
We say, once you get past that point, that's now big data.
Or at least that is one definition of big data.
Notice how it doesn't have a size.
So we don't say, once we get to five terabytes, now we're at big data.
When we get to a million petabytes, we're at big data.
There are those fantastic arguments where people keep shifting the goalposts.
That's big data.
We are all generating data all the time.
So those goal posts do get shifted, but the definition also does vary.
So think about Plymouth University as an organisation.
We have got how many courses?
We've got three faculties each with, I think three schools.
So it's nine schools.
And then say we've got probably, you know, 500 students on average on each year, maybe for those courses, some bigger than others.
Business school's quite large.
All the staff members we have here, it's a lot of data.
If you're a one man band builder who's just got invoicing and bills and accounting to do.
Google, on the other hand, looks at our data and laughs at our data.
Big data we are minuscule in comparison to something like Amazon, Google, ebay.
So bear in mind when someone says, oh, I've got a Big data set and maybe they're talking about half a million records that might be big to them.
So means different things to different people.
What is really important is that we handle this data properly.
Hence why you're here.
So too large, too complex to be dealt with.
You know pivot tables in Excel.
I'm going to do a lot of hating on Excel.
I'm really sorry.
If anybody loves Excel, you can shoot me afterwards but I really don't love Excel.
It's useless.
Know good old government during COVID testing recording all the tests as rows lost so much data because they didn't use a database.
Yeah.
So you want to join me in my Excel bashing?
Feel free money entries so greater statistical power.
So well not everything we're going to have we all find out what we call about black sheep and gray sheep and you'll find how they cause problems when it wants to predicting things because they go against the norm.
Large data.
If we can have more of these black sheep and gray sheep we've got a more chance of picking them up.
Don't have a formal definition of it.
Bit like when I taught AI.
We don't have a formal definition of AI.
Don't have a formal definition of big.
Daisa There we go.
But we are talking about what we call structured.
So nice columns.
Neat.
Has fantastic.
What we call metadata behind it.
If you've got a sensor that's doing temperature, we know it's going to be degrees Celsius or degrees Fahrenheit.
There's a lot of information behind it.
Or semi structured.
Maybe we've got a bit of sensor data and then we've got some opinions of people who happened to be in that place at that time.
Structured, unstructured or completely unstructured data.
So think people's comments on Reddit and you'll kind of get the idea of the kind of data we're talking about there.
Big data tends to mainly focus on unstructured data, but we're going to do a bit of both.
Mainly because unstructured data is the more fun stuff.
Trying to pick apart people's opinions and try and understand what they mean.
And how we can relate that to concepts, products, things like that is quite fun.
So we did run an experiment back when we were allowed to have access to Twitter data.
So January 2017.
Happy New Year.
We collected all the tweets that had that as a hashtag over 10 consecutive days and we got just over 50,000 tweets in that year.
Fantastic.
Donald Trump was elected president for the first time proof that you don't learn from your mistakes sometimes.
But hey ho is here we go.
In those same 10 days where we got our new Greers resolution tweets, 13 million tweets were sent about Donald Trump.
Yeah.
So looking at different topics will gather volume of data in different ways.
Some topics are more emotive than others and garner more talk than others, both positive and negative.
So Big Data has been around for a long time.
A bit like AI, but more popularized in the 1990s when there was a pretty much boom.
Anybody old enough to remember floppy disks?
Yes, yes, excellent.
I don't go out.
It had to go from floppy disk to CDs.
Now we're talking about SD cards, cloud storage.
And that rise has happened pretty, pretty rapidly.
So the size of what we call Big data is changing very rapidly.
There was somebody who did their PhD and it was on 15 floppy disks.
We can now hold PhD thesis on one memory stick.
But then we decided to get a bit more technical and in 2018, say, like parallel computing, when it's too big for one process and we need to start parallelizing, that's where we draw the line.
It's actually quite a nice line to draw because it made very clear where that statistical power from computing comes into play.
So computer science kind of then took over from that bit, but the challenges then arise.
So how do we capture that data?
Temperature fluctuates all the time.
Putting sensors in to capture it, that's where we're going to need it.
How do we then store it?
Storage is expensive and if you've got lots of data being gathered a lot of the time, you need somewhere to store it.
We have Plymouth Marine Laboratories.
I say just down the road, I'm gesturing in this area as if you all know where Plymouth Marine Laboratory is.
They currently have five or seven petabytes of data storage and they're nearly full from the amount of data they collect.
Completely mind blowing.
How do we then analyze that data?
If we've got four or five columns on a spreadsheet, we can go, oh, maybe I'll just do some pairwise plots.
I'm giving you Data with over 400 variables for your coursework.
How are you going to decide which ones are relevant, not relevant for your analysis?
How do you even picture all those different variables?
That's a lovely question for you to answer for your coursework.
And visualizing it.
We say, a picture's worth a thousand words.
Make sure we can see it and then learn from it, predict the future from it.
What are the best algorithms to be using.
So this.
Anyone who's done the machine learning course in the first semester, this will build a little bit on their techniques.
Anybody who didn't do the machine learning, you will have the code provided for you.
So please don't panic about that.
So what is we tend to use the two words kind of interchangeably, data and information, but are they really the same thing?
When we talk to big data analytics, we are talking about data, raw, unorganized statements of fact.
But they need to be processed, they need to be organized and made sense of.
So that way we can then draw information from that data so that it means something to the user, it has something of value to the user.
So when I say data, raw, unprocessed, bit of a mishmash information is making sense of that data.
And how we make sense of it is really important.
This is my favorite meme.
I love this data randomly sitting there in its database, unconnected, just doing its thing.
When we start processing it and understanding that data a bit more, then we can start to draw information from it, which gives us knowledge about the situation, about a certain population or a certain product, which then enables to give us insights into that situation, that product, that population.
So from that we can draw some wisdom and go, okay, so we know this about the population.
How can we make X better for them?
Or how can we make sure that Y doesn't happen again and detrimentally affect them again?
What we're trying to avoid is this last one.
Now, as much as I do love a purple unicorn, as an ex person, we don't want people drawing crazy ideas potentially from cherry picked bits of data or cherry picked bits of information.
One of my favorite ones that I think represents this beautifully came out of COVID studies.
So there was a claim that having the COVID vaccination made you infertile.
Anybody here hear that claim?
Got some nods.
Excellent.
That claim came from a study where they doused rats.
So think about the size of a human because of a rat.
With the 1000th times the dosage you'd give a human, tiny mammal, human, 1000 times the dosage they put into a rat.
And weirdly, yes, the protein did end up around the reproductive organs of those poor rats, which if you're going to douse them up that much, is going to end up somewhere.
And of course made them infertile.
But that's where that claim came from.
They cherry picked that fact without saying, well, one, it was a rat and two, it was a thousand times the dosage of a human.
That's often the most dangerous old conspiracy theories, they have that tiny bit of fact in them which you can latch onto and then people run with it.
So as much fun as conspiracy theories are, try to avoid them, please.
So big data 5B's true at time of recording, we could go to 6 at some point.
Volume, velocity, variety, velocity and value.
So big data is high volume, high velocity, high variety.
And we like to do it, keep it cost effective so that we're not spending too much money on it, but it's actually useful and it's going to give us some insight for it.
So volume, the amount that we're gathering.
Okay, so terabytes, but zettabytes.
Huge, huge amount of data.
We as people create 2.5 quintillion bytes of data every day.
Crazy, isn't it?
Think about what you do in your everyday life.
You might have commuted in to university.
We've probably got smartphones on us.
I know I have.
It's mapped your journey.
You pay for something using your card.
It knows when, where, how much.
Got your bank information?
I like to pace when I lecture.
It's picking up all that information, hopefully losing a bit of weight at the same time, but there we go.
It's crazy to think just how much data we generate all day, just ourselves now with smart devices that's.
I think this is probably very conservative now to be fair, now that we've got watches and rings, that's probably very conservative estimate.
But it's actually 10 billion.
10 million, probably more than that now.
So velocity, speed at which we gather data.
So gone are the days where colleague at PML was saying that back when they started collecting data.
So I've given some of this data to the second years to analyze for their AI coursework.
Goes back to the early 1900s where somebody would row out of the sound to a particular coordinate, drop the bucket into the sound, get the water to analyze and come back again.
So may have done that probably once a week, probably even too much for that data.
Now we have Poise sat outside in the sound gathering data every second, every tenth of a second.
Very, very quick.
I've not done it.
I've not gone viral yet.
But it doesn't take long to go viral these days, does it?
You get a good TikTok film or a tweet where you say something incredibly stupid.
The Internet is not going to let you forget anytime soon.
Quite a bit conservative now, but every 60 seconds, about 72 hours of food footage is uploaded to YouTube.
You probably all contributed to that at some point during your degree having to do videos for that.
And we can now.
I hate the word real time.
It's not real time.
There is always a delay.
And I hate when people claim that it's real time because it's not.
It's pretty close to it.
But of course there is a delay.
But we can analyze data pretty much as soon as it comes in.
Doesn't even have to touch a database anymore.
It can just get to flow through our systems and get information out of it.
Variety.
Gone are the days where it had to be numerical data or categorical data that could be stored nicely in relational databases.
We can now store text, we can store video, we can store images.
So the variety of data that we have is a lot more these days.
Is it any better?
But I think it is telling that 80% of the data that we look at is unstructured.
We can bring together data, different types of data.
So you'll find that in the annual population survey you've got categorical nominal information.
There aren't the same variables in each of the two data sets.
So you'll have to decide, you know, which ones you're going to keep, which ones you can infer information from.
But we have social media conversations, photos, sensor data.
Of course, social media conversations are always a fun one as analyzing text is still a massive challenge and a huge open issue.
Because computers don't understand sentiment.
The amount of times that there have been Twitter rants because somebody has not understood sarcasm beggars.
Well, Porter doesn't beg a bully.
Fair.
So messiness or trustworthiness, depending on what kind of data you've got.
So missing values.
People that have input values incorrectly.
I used a fantastic example of this with my engineers a few years back.
So I'm going to pick on the males because it's always the males usually aged between 12 and 14, sometimes 15.
But height and arm span should correlate.
You should be able to predict somebody's height from their arm span or vice versa.
There is a linear relationship between the two.
However, if you pick males, you can always guarantee that there will be a handful of them where they claim to be 2 meters tall with a 20 centimeter arm span or they're 15 centimeters tall with 150 meter arm spans always.
So can we trust it in the case of the 2 meters, 20 centimeters?
Maybe it's a typo.
Maybe they're supposed to put 200 centimeters so you probably could alter that one.
Somebody who's saying they're 15 centimeters with like a 6 meter arm span?
Probably not.
They're probably just being stupid.
So how do we know to trust it?
How do we know how messy it is?
What mechanisms do we have to deal with that?
Big Data tends to be less controllable.
Even with surveys, you expect people to answer them nicely.
Quite often they don't.
You'll see from the data I've given you for your coursework that there are a lot of missing values in that stuff that's generated via social media.
Twitter.
I refuse to call it X. Twitter abbreviations.
Wtf.
We all know what that one means, but either computer can be able to pick that one up.
Other short ones, typos.
Did that person really mean duck in that tweet?
Probably not, to be fair.
Colloquial speech.
We've got a fantastic joke in the British that if you travel four hours, the accent has changed six times and we've got a whole new world for a bread roll.
Could be a bap, could be a roll, could be a bun.
Who knows?
Depends where you are in the uk.
So those colloquial speech.
Reliability of content.
So of course you trust everything I say.
Not a sonic folder if you latch it.
But yes, I try to give you as reliable content as I can.
But Big Data is a changing field.
It does get updated.
So even by next week, some of what I say could be completely outdated.
Whereas somebody like Donald Trump, we trust every single word that comes out of his mouth.
He's such a reliable source of information and facts.
Yeah, exactly.
So checking the content and the accuracy of content, does everybody actually make sure what they are saying is correct or not?
Again, bias points of view come into it.
But we can now work this data really, really quickly, which is quite nice.
The downside, if you're making decisions with bad data, you're going to make bad decisions.
Okay, so it was estimated that it cost the US economy US$3.1 trillion per year dealing with bad data.
That's not a small number, still a big number.
And then when they're putting tariffs on every country and you've got less money to deal with, you don't want to be losing any more debt money than you do.
So making bad decisions, having bad data means you make bad decisions.
And okay, do we care if the economy crashed by 3.1 in the USA last year?
Probably not.
Do I care that 5 million people were misdiagnosed with cancer due to bad data sets?
Yeah, I care a bit more about that one.
So it's not just economy, but health Societal impacts that bad data can have.
Money at the end of the day, apart from fat bottom girls, money makes the world go round.
So how much does it cost me to gather this data?
How much does it cost me to process and analyze this data versus what's my monetary reward?
At the end it is the big bad horrible bit.
Hence why I say in academia because I don't have to worry too much about this side, but in businesses it is that bottom line.
How much money do we have to spend on this?
How much money will it either make or save us?
So a nice success story was in aircraft manufacturers.
They used big data to predict engine events which cost a lot of money for airlines.
But with that analysis, 97% accuracy, managed to save themselves some money, $63 million by doing some big data analysis.
If it cost them 100 million to gather that data, probably not worth it.
But it didn't cost them that much to gather that data and then analyze it.
But a lot of money saved.
There are additional characteristics.
These are not classed as the 5Es because they haven't found any synonyms that begin with for them yet.
When they do, it'll probably go up, but exhaustive.
So are we capturing everything or not?
Let's say humans, for example, if you're still using cache in any way, shape or form, they're not capturing all the data about you.
You're keeping some bits hidden.
Are we getting information from all the available data sources?
Sources large, LMMs, ChatGPT, Copilot.
They are trained on specific data sets.
So do they have sight of everything?
Finely grained and uniquely lexical.
So looking at the individual specific elements of them, how are they collected?
Are they properly indexed and identified?
Do we actually know what this data is?
Relational?
Is there data that we can join, collect together and make sense of it in that sense?
So not quite the same for anybody.
Comes from a statistics background as codependencies or co linearity because that's where they have a direct relationship.
You can infer one from the other.
This is more about general relationships.
Can we add to it or change it?
Easily.
Quite important with Spencers collecting data rather quickly.
And in terms of storing it, am I constrained?
Do I have X amount of storage and that's it and I have to chuck out the old stuff, make way for the new or can I expand it to accommodate everything that I want to gather?
Cool.
So if I, I want to do this, then we'll take a break and then I'll start and show you.
Neo4J.
So Neo4J is the database software I want.
A lab machine.
So hopefully it is.
I've already got it installed on my PC.
I'm using the Desktop Edition.
There is also a Community edition, so just be careful which one you're using.
Desktop is the university standard one, but there are also the community version and there is also the cloud version as well.
So you could avoid the cloud version because that one really is a bit weird.
That'd be great.
Once you have installed it, it will go through that little checklist and you'll get a landing page that will look similar to this.
So you'll have an example project and a project.
As you can see, I've already got two databases set up because I have done the solutions.
They can base for your set sizes and the refer element and hopefully that won't be needed.
So any project you set up will be here on that left hand side.
If you don't like what you've made or you know it gets to the end of March and you've passed the exercises, you can press that little red bin and delete it forever from memory and never ever have to go near it again.
So if you've already got your database set up there, you can just click onto those but refresh and new.
So weirdly we're gonna press new and we are going to be creating a project from that.
You can then rename your project.
I can't spell, so I've gone for something really exciting first class example.
And to this we need to add a local dbms.
So database management system.
We're going to be doing a local one, not a remote one.
It will come up with a name for the database.
So if you want to give it another name.
So for the set exercises, you might want to call it Set Exercise Database.
I'm lazy and I don't change the name.
It doesn't actually matter whether you change the name or not.
As long as you've got the project sense of named, you'll always find it again.
Because it's a local database, the password doesn't actually matter.
So I tend to use the same really secure password for all of them because it's held locally.
So if somebody gets your machine and gets into your machine, you've got bigger problems than them getting into your COMP3008 set exercise database.
Personally, you can control the versions, so just be careful.
It will default to the the latest one that is installed.
There are some again, you know, weird errors and things that get picked up if you're using later or earlier versions.
So I would recommend you stay with 5.5.
3.0.
If this one doesn't work for you, please feel free to change it to earlier ones.
Just let me know with a comment in your submission file so I know that I can.
I can change my database to the same version and then we'll be able to run your code.
Okay, so when it comes to the submission, it's a text file and I will be running your code to make sure it runs.
So if you are using a slightly different version, let me know so that I can make sure my settings match yours so I can get it running.
And then we just click Create.
It does tend to take a little while to set up the new graphical database, so I'm just kind of trying to waffle on and set some time up before.
No, it's going to defeat me, unfortunately.
So don't be afraid that it does take a little while.
It does take a while for it to set up.
There we go.
It's quite nice because you have your database name, you'll be able to see what version it is.
Once that is set up, then you press Start.
It will open the command line and then close it again.
It's really quite clunky to get it working.
That's why I wanted to take you through and make sure everyone's happy starting it up.
That yellow strip will turn to green once it is active.
Oh, blue, blue, active.
Up and running.
Please stop your database before exiting Neo4j.
If you just exit Neo4j without saving or without stopping the database, it does cause errors with ports.
I don't know why it does.
Okay, so please always ensure that once you've done the task you're working on or you've gotten sick of whatever it is you're doing and you need some time away, please press stop and then exit Neo4J so that you don't get any port issues.
If you do get port issues, then the only way so far I found to solve them due to the University networking system, is to delete the database and the project and start all over again.
Now, if you're on exercise four of the set exercises, that's probably going to make you cry, so please stop and then exit to avoid tears.
I do have tissues and a sympathetic shoulder.
If that does happen, let's try and minimize that.
So now we've got it up and running and active, we click open and it does open in a browser.
Although it says local, it does open up in the browser.
It always opens up minima or not at the full screen.
So I just like to make sure I've opened it up to the full screen.
This bottom bit just makes.
Just tells you that you are connected to a server, what port you are connected to, and the user that you are.
And then some nice information about getting started with with Neo4J.
These are links and will take you to external should you want to.
I. I tend to just delete that because it takes up space on my screen and I don't like it.
So commands.
It's nice and chunky, so.
Oh, where's my E?
So anything that is a Neo4J command will appear in green.
Okay?
So try to avoid variable names, like for in things that will also be variable names because it will highlight them as green and then it won't let you use them because it's protected.
So green for any variable you want to delete.
So let's say I spelled creates incorrectly, missed off the A.
The.
The cursor highlights the one you are on.
So cre.
I will need to put the right place.
No, I put it in the wrong place.
So I need to put the A after the E. So make sure the T is highlighted and then put the A in.
The amount of times I've deleted, I've gone too far on because it's highlighting the one you are on.
So then I'm going to get these brackets wrong.
So I'm opening that back out.
As with other coding languages, be careful with your curly, your angled and rounded brackets.
They all do mean something.
So for create, we just use the rounded brackets.
And nicely for the lazy individuals among us, it puts a second one in for you.
Okay.
So it will always close your bracket for you.
Okay, so try not to delete those brackets.
It's helping you out here.
It's closing them for you, which is amazing.
So let's give our variable a name.
I'm going to call it name.
And then we need a.
No, see, it's picked up on something that I've already done.
Why have you done that?
Go away.
That's the wrong mess.
Go away.
Right.
My crate's gone now.
Crate.
Come on.
Why.
Why are you now not typing?
Why have you decided that you hate me?
I mean, I hate me at the moment.
Please don't.
You hate me too.
So it does have memory.
So flicking up and down with the arrow keys will do first and last commands.
It does do yoke, control and paste.
If you need to go to a new line, please press shift and enter to get the new line.
If you just press enter, that sends it through to the Console and it will usually give you a lovely error message.
So here it doesn't like it because basically it's incomplete.
I've told it to create and it's gone.
Create what, Lauren?
You're not giving me any information.
So it's just the invalid input is the nothing at the end of create.
It expected those rounded brackets.
Some of the error messages that Neo4j gives you are helpful.
Like here it's told me the command has stopped and it was expecting a bracket.
Sometimes the error message it gives you are not helpful.
So not wildly different from any other computing programming language, to be fair.
Some error messages helpful, some not.
Oh, I'm going for that lowercase name and I can't spell my own name.
This is good.
So I'm creating a variable with name and then the label name for that is it's Lauren, it's me.
You can press the blue arrow or enter that will send it through and then if it has been successful, it will tell you what it's done.
So we have added one label, we have created one node, and it took 8 milliseconds for us to do that.
We then can see that node in the database.
If I click on that, there I am.
That's my little node in the database.
We can hide different nodes if we want to.
You can change the colors as well if you, if you really want to.
If you don't want to look at it as a data, as a graph, you want to look at it in a table.
It looks very much like a JSON string.
Lot of non relational databases.
So MongoDB is the other big one.
Looks like a JSON string, has all the information in there, or you can get it as a nice little text table.
So if you're doing any queries for results, which you will be doing for your coursework, if you click text, you can just copy and paste this into your text file for your submission so you can see results graphically or as text, which you will need to put that text file text input into your solution.
So we can then add another one name.
And let's pick.
I often pick on Rory.
He knows I pick on Rory.
The great thing is he's semester one, I'm semester two.
So I can do it more than he doesn't.
For me, it's great.
So I can create a second node.
So we've got two little nodes.
We've got a little node for me and a little node for Rory.
If I want to view them both, I can see them there.
Or if we pop them as a graph Two little nodes sat there in the space that is.
So that is how we then start putting data into a Neo4 database as simple as created curve brackets, variable name and then the label for that variable.
If it's done correctly it will tell you that it's been updated but you can always look on your database information and make sure that they are in there.
Of course for lots of data inputting each one individually is not very time efficient so part of the we'll be going on to looking at loading in CSV files but the first kind of activity that I want you to be doing today is to get used to create putting these nodes into Neo4J and just getting used to the layout and setup of Neo 4J.
There's a couple of exercises on the DLE for today just to start that playing around with this syntax and getting used to what's going on that is all the talking from me for today.
I'll pop the attendance code up so if you want to.