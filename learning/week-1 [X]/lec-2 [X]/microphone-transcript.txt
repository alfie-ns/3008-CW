and then the load CSV function because I want to show you preferably how I want you to load the data in the database to make my life a lot easier, hopefully make your lives a lot easier as well. So by probably about quarter past half past five-ish, you'll be able to know exactly what we mean by know SQL. You'll know what a graphical database is, and you'll be able to import large CSVs into Neo4j, ready for the same element. So a bit of a recap from Tuesday. Big data is an ill-defined term. we've got the five V's at time of recording anyway but nobody actually agrees on what big data is where is that cut off point some people say it's where we need parallel computing some people say it's when we get to a certain level of data size so not very well defined spoiler alert a lot of my courses are not going to have very well defined terms AI isn't a very well defined term big data is not very well defined You'll find that NoSQL is not very well defined. It's a bit of a theme of everything I teach. So there is this ambiguity, which means we don't laugh at people when they come to you and go, I've got six USB sticks full of data, that's big data. Then it is. NoSQL actually arrived by accident. It's not a clever acronym for anything. it was just coined by the hashtag as we all see and they kind of ran with it. And so because of that we've got no exact definition of NoSQL. There are common characteristics that NoSQL languages, databases have but no precise, stick it in a definition in a dictionary. That's the word I'm looking for and frame it. So, big data management. Anything that helps to support big data management is a big management strategy. It's a bit more than just sticking stuff in a database. So when it comes to big data, we have to think about who can get access to it, where are we doing it, and why are So it's the organisation, the administration and the security of the data. Hence why this is a fibre module as well for you on fibre course. And it's large volumes generally, but both structured and unstructured. So it's amazing what information we can glean from unstructured data. So it has to be a management system that can handle large volumes of data coming in potentially very, very quickly. And therefore needs to be scalable. Very scalable, very quickly. And from different formats. So we could be taking in video data, we could be taking in sensor data, we could be also taking in text data. So you can be able to store that nicely as well. Challenges we have with big data is that we don't always want everybody to have access to that data. We have data silos. But if we have data silos, we have duplication. Now, this place is terrible for it. As soon as any organization gets past a certain point, there is horrible duplication. And if there's duplication, there's risks. If you've got data stored in different places and they have different levels of security, slide issue, especially if it's essential GDPR compliance is required. As data storage grows, generally you're taking up more of your server space, you're then slowing down the capacity of your network, which isn't great. I've got a project with lots of different companies actually. most biggest one is pool harbor they have given me five years of AIS data for five of their vessels it's approximately over 800,000 AIS entries I've handed that over to another partner who's put into a dashboard it takes somewhere between two to three minutes for any chart to load which is a sheer amount of data. So large data, you're going to have a slower system. And large data tends to be more complex. You're gathering more orbits, you're going to have more missing values, people are going to be misinterpreting questions potentially, trying to match text to images, present to data, provides a challenge in its own. And then maintaining the quality. If I've deployed 10 sensors across the city, how can, and they're sending data back once every second, can I sit there and monitor every bit of data that comes back every second from those 10 sensors? I can't. It's ridiculous. So being able to trust that data, how do we know that all the information coming in is true? Deep fakes made through AI are a brilliant example of this. If you're collecting video data, how do you know that one of the videos you've got isn't a deep fake? Manpower. So the number of experts you need in data analysis grows with the amount of data you have. So that's good news for anybody who does want to go into a data analyst role, because we're all producing more data all the time. Therefore, lots of roles in this area. good news maybe not such a challenge and then there are three affectionately call them the dinosaurs so the ones that like to use paper-based systems trying to get them to convert to technological solutions is a bit of a challenge so yeah I had a fantastic personal example of this I had to have my gallbladder removed and I've got notes from my GP, I've got notes from the hospital and then they sat there and there was actually no mention of my second child on my hospital notes whatsoever, I had him at home, so as far as the hospital concerned, never had him, but they were looking through my notes and went oh you had a large dose of anaesthetic on this date and I was like yeah I was in labour and I gave birth, they went Yeah, that would do it. That's the same hospital, two different sets of paper-based notes, not joined up. Yay for the NHS and healthcare. But it is scary that we have massive organisations like the NHS who are only just starting to adopt electronic healthcare records. And even then, the linkages between them is not what we would like them to be. There are other places that are actually streets ahead of us. but if we get it right there are a lot of benefits we can access data a lot quicker and a lot more structured way so if we're doing marketing we can tailor marketing a lot better who is on social media seems to start googling something you get the ads yeah big data looking at your search history tailoring your ads to you hoping to make a hit so unfortunately for them it's my husband that played golf, not me. Better customer service. If you've got linkages between data and you can see those relationships, you can treat your customers better. So take the NHS, for example. I was a little bit concerned that they had no knowledge of my second child or that they realised that the anaesthetic I had was due to giving birth to my first child. And that's a healthcare provider. We should be able to trust them. And having those better database management systems can create without linkages. Gives you more accurate analytics. Bad data, bad decision. You can get good data stored correctly that you can get to quickly. You can make good decisions. And then of course there's the competitive advantage, which anybody in business is after. Processing is a form of information processing. So we're taking those pixelated bits of information and drawing that world map and getting that information out of it. So it is everything. Collecting, exploring, managing from multiple different sources. The two main ways we do that are batch processing. So essentially this is what you will be doing in your coursework. You'll be batch processing where we collect a large amount of data over a particular time period and then just send it all off to get filed wherever it needs to be done. So this is for non-time critical evaluations. So maybe you can get poor detailed data. Stream processing, again, as real time as real time can be. So this is where everything just flows straight into that database. The processing that all happens as part of the pipeline to put it in place. So this is for those time critical kind of analysis that we need to do. So things like an ER can make use of that. And storing it. Because we're storing types, we need to make sure that we can store diverse types. So you've all had experience with SQL and relational databases. Can you imagine trying to store an empty file in a relational database? Not too easy. Probably the first example you might have seen is the Minus dataset. So I know I used this for the AI course last year. So that is the pixels, grayscale of the numbers. That is stored as grayscale numbers in massive, great, big arrays. What if I could just store it as a picture, one file? Would that not be much simpler? If I wanted to load in 15,000 digits, I do not want to have to store 15,000 tables, 64 by 64, or 128 by 128 of numbers. So we can integrate more sensors, and there are sensors everywhere now, are they? and we've got a rare sensor in the garden. So it seems the big data storage are graphical databases, document tiles, our databases are much better at handling data storage. Is everybody still with me? Excellent. Thoughts like me. So different types. in no particular order to be fair. So distributed file systems kind of says what it says on the tin. The information is stored across several different servers across the network. And then multiple servers can access them, multiple users can access it. Security kind of point of view, maybe not the ideal one, but yeah, can do. No SQL, the ones that we're focusing on, they are the ones that say, we see your tables, we see your rows and columns, and we reject them. They are not useful, throw them away. Let's get loose and fast with data and store it however the hell we want. Data warehouses, houses for data, essentially, they will batch data through and store them, So they will perform the processing, the cleaning, and some analytics as well, depending on what sort of warehouse you go for. There are two different ways that they will then send the data through. But it's the transformation of the data. It will either take place last or in the middle. But that's essentially the only difference between the two different pipelines you can get there. Or object storage. taking inspiration from object orientated programming which I'm sure you all know and love maybe not so data is treated as a unit in this case and each one is stored as an object so it's got a nice architecture for handling that unstructured data pros and cons well pros are data driven decisions Smarter, faster decisions, more data available to us, whether that is begotten through ill-gotten means or not, and I will show you how to collect data. It's not technically illegal. It's a grey area. So how we collect data. Making informed choices. If we can see all the data, we can see the relationships, we can hopefully make better decisions. scalability you don't have to worry about introducing new schemas or updating a schema and making sure everything is okay and efficiency but there's always a but comes at a cost storing it you're getting more storage just costs money I mean I don't know if you students have looked and seen how much storage costs like in the cloud but people are charging ridiculous less amounts of money for it it is resource intensive and demandive you know you've only got to look at the data warehouses and the amount of carbon footprint that they have which is why some people do have moral and ethical standpoints against things like chat gpt that uses these large data warehouses data security risks depending on the type of database you go for you have to make sure everything is secure make sure your GDP are compliant otherwise you're gonna be at the foot of a rather large bill and then complexity so as we'll see from some of the graphs that we produce from are we not that complex data so we'll look at the ibm data set we've got the four movie stars we've got uh runtime rating we've got synopsis release year i think genre but you can create some highly complex relationships just from those six or seven categories and then of course you've got to be explain it somebody needs you to jump onto a project or you're jumping on their jump onto your project you have to explain how everything's gone on if it's complex they're not going to appreciate that whatsoever so data privacy sometimes information privacy make sure you are handling data properly don't leave it on a memory stick in the middle of a train all those GDPR ones but sometimes we get a bit of a false sense of security that we're sat behind a firewall must be safe that one our servers behind a firewall but you can use our VPN and get in that way and you have access to a whole load of information and using the data we will have a session a bit later about the ethics of using big data one of the reasons why I don't have a smart meter is because I know exactly what I would do with that data so I don't want them having that data on me so making sure we are using it properly if We do have GDPR which says you must be upfront and honest and tell people how that you are using their data. But do we all trust the corporations? Yeah. Yeah. Yes. Love it. They're all telling us the truth. Yes. But it is powerful. We can tailor things better to our customers. We can hopefully react more promptly when there are issues. But it does come with responsibility. With great power does come great responsibility. So we need people to trust us like you all trust me. There are the illegal implications and we want to avoid any financial repercussions. And then, of course, ethical responsibility. We shouldn't be doing anything too dodgy with the data, hopefully. so although Neo4j is not the only type of non-relational database that is out there it is one of several it's my particular favourite because I come from a mathematical background so I like the fact it links to graph theory that's why it's my one of choice but there are others out there that are widely used So big table, it starts with a very sparse populated table, but then you can add to it and it will grow to billions of rows. So you can store terabytes or petabytes of data there. The key values, people like those because they feel more familiar. They've got key values, very similar to your relational database, and they have a nice dictionary or key value that goes with them so you can use filters with those it's just more aligned to what people are used to with coming from relational databases so I don't know if anybody's met any of those some of them are paid for unfortunately and then document based ones look like JSON files MongoDB I think is the most common one that I've come across in my own research and through various student projects over the year the multiple different basically documents that can sort different fields so you can have similar fields amongst them some of them can have different ones it just gives you that greater flexibility especially when we're dealing with the unstructured stuff. So why no SQL? So it started off as a Twitter hashtag and it was chosen for a meetup. No idea why it was chosen back in 2009. And they were discussing new databases so it became very clear at this point that we needed something different to what had gone before. especially in particular they were looking at open source a lot of databases were hidden behind paywalls and you were stuck with what they gave you so they wanted to be open source to allow flexibility talk about unstructured data different formats kind of key here it's So you've got distribution and non-relational. So 2012 they decided to say it's an accidental term, no precise definition. I don't think it still has a precise definition. That's the wrong keyboard. So what it's supposed to mean is not only SQL. SQL is not the be-all and end-all. so that when we are thinking about creating a database sometimes our minds go straight to those tables and columns and rows because they're familiar and they're timely and they're ordered and that's nice but it doesn't always work so these type of databases are not actually new they have been existing since the 1960s but it took the 2000s for them to really become popular. Think about what happened in 2000, the boom of the internet and videos and YouTube. Kind of makes sense why they became a lot more popular. The important thing to note is that although it's not only SQL, it still does support SQL-like languages. You've seen a little bit from Cypher, with the Create, but it does look SQL like so it's not a complete change from and still does support it I think a lot of new technologies not just databases do receive a lot of pushback if they're too different from anything else so the idea here was to make it not so different welcoming and not adopted. That's my completely wrong. Although we have no definition, we have four features. So any NoSQL database will have these four features. So it will be non-relational. It will have no fixed schema that you do not have to update when you add new data. It doesn't have rows, it doesn't have columns, it doesn't really have a table. at all. It needs to be customer friendly, so it can be distributed over a network really, really easily if you need it to be. Open source, flexible, be utilised by any user for any need, for any purpose. And that code is then made available for other people to go, oh, yeah, that would be useful for mine. Fantastic. So much more sharing and collaborative than perhaps databases have been in the past. And then 21st century web, so quite a little bit outdated now. But that was the current state of the web. So this is after the dot-com bubble, where content on the internet started to go to be more user-generated and have more user-friendly interfaces, rather than just being, here's my website, navigate your way around it how I like you to and be done with it. So why why backward databases? Well there are limitations of relational databases. We can't store certain types of data in them. We could store text data they're going to take up a hell of a row but then how would we analyse that? How could we draw that into a query? They also, you'll find, really, really slow, surely, because you need to look up the tables and match them all across. It does become very slow when you get large data you're querying. Social networks, they do not play nice with relational databases at all. Some of you are on LinkedIn, so that fact will become known when we start doing queries. You're on other social networks and you follow each other, but not all of you will follow everybody else. And some of you will have different interests to everybody else. So how can we map those areas of interest? How can we put that into a column-based database? And real-world applications, the one that we're going to focus on mainly is recommender systems when it comes to pushing content onto your timelines, Netflix telling you what you're going to watch next. this is where these databases so multi-joined queries then if we try to do a multi-joined query in sql out of interest yeah how did you find it I mean if you get a little complex to formulate it together but if you've got an idea of what you're trying to achieve and how it's all mapped out it's not so bad but if you're going in blind it can be your way struggle Yeah, so graph databases aim to try and reduce that. If you know the relationship, you can find it a lot easier. So schema, your schemas have to be really rigid, don't they? We relate to our databases. You have to know what's going into what table, how that table relates to another table, what's your primary table, what's your key. We don't care about that. We don't care. We're living loose and fast. There are limitations in graph-like queries. So if we did want to do the multi-join web, looking at family trees, or if you're looking at supply chains, anything that you could map out as a tree in your brain, traditional is going to hate you for it. And it gets slow, as I've already alluded to. Whereas because graphical databases use tree-like structures, they use a mathematical way of searching for them, which is a hell of a downside quicker, which is nice. So what is it? Well, we're going to store data not because of what the data is, but because how they are connected. So we still have the information about everything. But if I'm interested in... I'm going to pick up my sister's job. She's a supply chain manager. So does she really care that she's getting, well, she does care, she's getting so much from this company, so much from another company, and they've got another delivery company, or would she rather know that they've actually, there's that chain that's going through, and it changes so long before it gets to her, that impact on her delivery for the project that she's making. That's the bit her business are interested in. The amount of time she's moaned because one of the bosses has gone, yeah, we can fill an order for a million units. And she's gone, whoa, no, no, no. I don't have the staff. I don't have the raw materials. They're like, oh, but, yeah, I looked in the data. She's like, no, no, no, no. You forgot lead times and wait times. So we can see how all things work together. So relationships between things, what works, what doesn't work. Do I have an employer who worked for a different business, maybe a competitor, and they came to me and I'm pleased about that? Or maybe my employer is going to my competitor and I'm not so pleased about that. We can follow how those relationships change quickly. That's how the Twitter algorithm works. We will look at that in a bit more detail. And rather than questioning the data, we're questioning the connections. Why is this happening? Why is everybody going here on holiday? Why is everybody doing this? So connections rather than actual data. So how they work? They follow that relationship. Think of like little ants, when ants find food. They're following those paths, which make them a lot quicker. They're pattern-based, a bit more like how we think. They follow that, which is quite nice. So rather than having heavy joins, I can just go, here's a relationship I know exists between all things I want to find. Let's just query that relationship and see what it gives me. Oh, I know that these nodes have these edges. Let's grab all those and find out. We can picture it. Who likes looking at really big, long tables until you go across the line? Yeah, me neither. But who likes looking at pretty pictures with nice colours? Yeah. You saw from the first session where you had those nodes with the different colours for the different tweets and the text in them. When we get to relationships next week, you can see how they join. But wasn't it fun being able to move them around and watching them wiggle? It's far more interactive and lets you really expand and when you get the bigger ones, when we do the movie database, please do grab the nodes, move them around, look at those relationships and see what's going on. Expand them out because it's that picture that gives you the information. So when to use them. If we have complex interlinked data sets, okay, the data sets that I've given you for the Eurovision one, they're not that complex, but they are interlinked. So one's information about the host countries one has information about the voting results and the other has information about the winners so they are interlinked how would i do that in a relational database probably very difficultly but you'll see how you'll do it for the coursework we can then see when we have a deep or a frequent relationship then this is actually one of the subjects for the query that you will produce as part of the 30% of wanting to explore voting patterns. Who gives lots of votes to other countries quite commonly? You know, the whole Greece-Cyprus thing, basically, I want you to find out. And then where we have evolving structures in data. So if I have a relational database and I have more data, I've got a new table and I've got to see how that fits in the schema and I've got to update my tables. whereas here we can shove the nodes in and create a new relationship and anything that matches the database just puts that relationship in for us we don't have to think about making sure those connections are there it does it for us it does the thinking for us if we do have tabular analytics then stick with your racial databases graphical databases are not the be all and end all a bit like machine learning it's great but not for everything So again, graphical databases, not fantastic for absolutely everything. If there are heavy aggregations, if we're pulling a lot of data together, again, not quite so useful there. And where we do have simple patterns, use those simple patterns, don't go for anything fancy. It's that analogy like how do you crack open a nut? Do you take the nutcracker or do you take the sledgehammer? You don't generally take a sledgehammer to crack the nuts. So pick the best tool for the best job. I'm just hopefully introducing another tool to you to add to your toolbox. So challenging when they are distributed, scaling them can become difficult. Making sure everybody's got that right relationship that is all linked up is important. Complexity. You can get really, really stupid with your queries and you can get very lost with them. So just be careful with that. It's a new tool. It's a learning curve. You had your first introduction to it on Tuesday. It's new. It's scary. Even if you've seen SQL languages before, it's a new tool in your toolbox. So there is going to be a learning curve. and why I'm teaching it a little bit differently this year hopefully it works and data ingestion and integration issues so some quite very things I don't doesn't always work perfectly and bringing the data into it and making sure that everything is okay doesn't always go to black but where do we use social networks they love it so Twitter creates and cool communities and that's how they create the two timelines in Twitter so we all look at how they basically create that not the for you but the recommended one which is full of people that you don't tend to always follow recommendation systems their Netflix, the scrolly bars at the bottom of eBay and Amazon. How do they know what you're going to look at next? Fraud and cyber security. Being able to see perhaps where cyber attacks have been and see where vulnerabilities are. It can lead to their prevention. And knowledge graphs. How we think as humans, so it has that AI link to it. So here is probably a really good point to pause before I go into loading. So there's a few slides I want to talk through first, and then we're actually going to do them together in Neo4j. So I want to show you how to load the data in and common pitfalls, but now is probably a good time to take a break because it's quarter to five on a Thursday. So it's 10 minutes, come back about five to, is that? Cool. And I'll check on the other room. Tonight's surprise, Brian has already made it to work. And then I was like, oh, I should probably do the same. The only thing I have is, you know the hashtags and the tweets? Tweets, yeah. Is there a relationship with the team? To the page, because to me, if you press on the hashtag, it should show you all the tweets that are tagged with a hashtag. Ah. That's what I saw. Right? This is the thing with related databases. It's what makes them do. Okay, yeah. ... we aren't going to want to sit there and do create a couple of hundred million times, potentially. So luckily, we can make good use of our friend the CSV and load it into the O4J. So one of the examples I'm going to be using is this data here. There's a lot more of it in the CSV file. So the two CSV files I'm going to be using can be found on the DLE in the folder, wonderfully named data for in-lecture examples, because I am completely unimaginative and like things to do what they say on the tin. So only five rows. I'm in this one. I can't remember how many columns, five columns, how many rows it is. So we need to tell you what we're doing. So we're going to load the CSV. We're going to load it with the headers. So it's going to get that information from that first bar. So make sure your variable names are in that first row. We're going to tell it the file that we're going to bring it from. We're going to tell the function. So there are a couple of different functions. There is CSV line with a capital L. And there is CSV line with a lowercase L. I'm not entirely sure of the difference between them, but whichever one you do go with, and I can see that I've done that mistake in these slides, is make sure it's consistent. So when we've got the create, like we did on Tuesday, we're going to create the nodes. So this time we're going to give them all the same names, and then this CSV line and every single one that comes after the variable names should also be a capital L. So I need to make sure it's consistent. What I'll do is when I'm demoing it shortly, I will deliberately make it wrong so you can see the error that will come up. Now, if we have very, very large data sets, there is a memory limit on Neo4j. To get around that, we can batch, truncate the data. So we need to include in transactions of, if the 250 is random, I've just picked that because it's a nice number, you could do it in transactions of 10 rows if you wanted, or 100 rows, or 1,000 rows. Whatever makes sense for your data, whichever will load it in quite nicely. Again, I'll do an example of how this works. with a cautionary tale of course. That's the wrong screen. The other thing we can do if you want to is to create an index. So indexes can be helpful when you are searching. They can speed up that search a little bit more. But of course with every benefit comes a price. The price here is they do take up that additional storage space. So you need to decide whether you have an index or not. Cautionary tale for your coursework. If you decide to have indexes, you must use them in your queries. You will be docked marks if you make an index and you don't use it. So if you want to, completely valid, do not worry about indexes in your coursework and just run your queries without them. This is the safer course to go on. I don't want to say don't use indexes, give them a go if you want to, but if you have them, you don't use them, you will be penalised. Consider yourselves warned. Creating them is quite simple. So we create an index name for a particular label name on that property. So you have a go at creating them within the lab sheet, so for the Twitter data that I've given you. You can create an index for the tweet on the tweet ID, and then you can search for it that way. I'm not a huge fan of indexes. mostly because they take up space, and space is valuable for data, so I tend to not use them. The game they give you in speed for the queries, sometimes it's just not worth that hassle. But play and have a go with them and see how you find them. So, quick summary of the slides, And then what I'm going to do is I'm going to stop sharing that screen. And I'm going to start sharing the screen. Oh, I'm on the Airforger. Wait for the Airforger to load up. Come on here. Oh, I know. You're too slow. Thank you. Oh, excellent, it's coming up in there. And everyone can see the screen. Fantastic. Okay. So, let's chuck that out of the way so I can see properly. Right. First of all, the important bit is getting the data in. There is a very specific way I want you to do this because it will make your lives a lot easier and you won't have to worry about file pathways at all. So I've got a blank project here. I'm going to create a new graphical database. And create that one. That will instance up my database for me. I want to make mistakes I want to show you the mistakes that can easily be made you can bring data in using a file path but I will be using this method which is a lot easier and And it also means that if you're working on a different machine and it's all linked up, you don't have to worry about it being in different places. It will just be in your graphical database. Ready to go. Why does this take so long? Come on, I've waffled enough. It should be broken out of it. It's not going to say. There we go. There we are. So getting the data real, what we're going to do is if we hover the cursor over the graphical database, you can see it says start, open and then three little dots. So we're not going to start it up just yet. What we're going to do is tap those three little dots and we're going to scroll down to open folder and we're going to open the import folder. so that will automatically pop up with the file explorer so this is the import folder directly linked to that instance of the graphical database I can go to the right week and see I've got the two CSV files there they are on the DLE download them, extract them and pop them straight to the download folder and literally just copy them into that import folder. So they're there connected to that database. You don't have to worry about any file paths. They're there. And I'm going to close that down. And this is when I'm going to start my database. So it's just a habit that I've got into putting my data into the import folder first before I start the database. Because I have a habit of starting my database, trying to import my data and it goes, I don't know what you're on about. Okay, fair enough. So we're going to pop that in there. It's a bit like when you send an email with an attachment, always attach the attachment first and then write the email. So you don't forget to attach the attachment. yeah a bit like my failsafe there and then we can open it so I'm going to first do the gaming data using just the straight load CSV and then I'm going to do the Netflix data with the transaction so the first thing to tell it is to load CSV with headers prompt then the quotation marks they will be this green colour because they see there's a function at the moment but what will happen is that. It will then turn brown as it realises that it is now a string. So colon, three forward slashes, three of them. Not two, not one, not four, three. Looking for the three. And then we can just pop the name of the file in. Remembering underscores and remembering the file extension. Like with most programming languages, it likes to tell you what type of file it's looking for. If you just give it the file name, it will behave like a stubborn toddler that's hiding behind the curtain going you can't see me I can't see you be very explicit make sure it is looking for the CSV file the nice thing about this is it like Python it indents so you can see which parts are all part of the same function task someone finds a better way of explaining that please let me know. So doing it as C S D with a capital L but lowercase I then did all create absolutely everything rounded brackets colon and then just give your node give the notes a name that makes sense so here these are more related to game releases on Steam so I thought they face games which related to tweet, call it tweet, Eurovision Song Contest Winners, I'm going to call it winners, I don't know, but a naming convention that makes sense to you. And then we need the curly bracket so everything within this will become properties within those little circles. First I'm going do is title, hold on, nicely as it predicts what I want it to do, so I do want to have a CFV line, dot and title, no not title, title. Make sure you know how your column headers look in your data, okay, so it is case sensitive. So if we had title with a capital T in the database make sure that title has a capital T I'm only going to do the first two columns so what this will do is it will ignore the other three columns in the database but just keep adding commas giving the variable a name and then telling it that you're going to load it in with csv and then semicolon at the end and click run or enter if it is successful it will tell you so it's created 60 labels so i had 60 rows data so I have 60 nodes associated with them and I have 120 properties I only brought in two columns of data so one node two columns two times 60 is 120 excellent all my data has been loaded in correctly so it's a nice way of checking to make sure that you know all your data has been pulled in correctly I wanted to run it let's run it again so let's bring in the sales percentage this time so I'm going to call it sales CSV line now in the CSV file percentage has a capital P I'm going to deliberately not do that so I should now have 60 labels 60 nodes with 180 properties if I type that in correctly but if I run it again the annoying thing is it doesn't give me an error which is slightly frustrating but I do only still have 120 properties if I change that now to a capital P and run it's still not liking me of course not how many nodes have I got see live coding it's great isn't it Why are you not? What have I typed wrong? What don't you like? I think I've spelt something wrong, which is why it's not going through. But you do get that nice confirmation when it does go through properly. As I said, it's irritating that it doesn't give you that message to tell you that something hasn't gone through properly. But if you're dealing with large data sets, are you going to sit there and go, well, I've created 362 million new labels and each one has got 472 properties. Are you going to sit there and want to multiply that? No, probably not. So for small data sets, it's a nice check. it is a bit irritating there isn't that kind of error that it can't find that one I'm hoping they might fix that in future but not going to hold no breath you'll also know if it's in right because you see I've got got my little games nodes there I can click on them and they've all appeared and I can move them around so the other data I had is the Netflix so this one I'm going to do in transactions because it's a little bit bigger this is where there might be a couple of different methods so I'm using the desktop application which is executed actually physically on my PC and I some of you are using the browser azora so the difference potentially is i haven't played around with the azora version is this auto in the version i have if i don't include auto it tells me i can't do stuff because it's part of a transaction and i'm trying to you know call things when i should be trying to push them or something it's very strange so in the version I've got I need auto that might not be true for the versions you're using okay if you do get errors let me know you send me a picture of the screenshot and I can troubleshoot that through with you try with try without the main space, make sure the space so that it knows it's a new one it starts off exactly the same load CSP with with headers wrong and we're gonna do file not in caps lock this time one two three I've missed off the ::Netflix_titles.csv using the Netflix data and if you don't put it in your import directory it's gonna throw it a bit of a hissy bit with you we still say as CS the line but now I'm going to introduce a new function called cool so what this will do is it will truncate the data for us so it's calling this function and doing it in transactions and curly brackets for this bit so anything that you want to call in the transactions goes within the curly brackets we have to tell it we're doing CSV line twice so again think angry toddler needs repeating so with CSV line with that function good old create we're gonna put some more nodes in rounded brackets colon name for the group of nodes carry brackets the first one I do is ID csp line dot show lowercase i and d and let's bring in the title so just bringing in those two so then after that curly brackets I'm going to tell it in transactions of let's go 35 that number could be anything you like you might need to play around with it because it will depend on the memory usage that you have so you can start big right way down and then we pray no it's I didn't like it couldn't learn the external file is that because I can't spell exactly right so making sure we can spell correctly is always a good one yeah I meant to make these on purpose good spot thank you so here we go slightly bigger data sets than the other one I've now got 8807 labels with that main nodes i've only created the two columns so we've just doubled up that number so that they're in there they are on the right hand side in this lovely peach pink kind of color and i can click on them and make sure they are all there so those are two different methods for bringing it in depending on the size you data set depends on which one you will need to be using and depending on whether you're through the desktop executionable like I am or through the browser one you might need that auto you might not I don't know why it's different but there we have it that's kind of all the top material for today there is a worksheet so you have a go at loading these databases in I'll go around with my scanner and anybody who wants to go there you feel free covered everything I wanted to thank you very much 