So you'll at least be able to start getting the data into Neo4j and then we'll start creating relationships from next week.
That's the master plan.
So why are we talking about non relational databases and particularly graphical databases?
Well, it's because we have large datasets now and traditional relational databases just don't actually cut it anymore.
The basis of these is called no SQL.
Doesn't mean we're completely abandoning SQL, as we will see.
Then we're going to look specifically at graphical databases and then the load CSV function because I want to show you, preferably how I want you to load the data into the database to make my life a lot easier.
Hopefully make your lives a lot easier as well.
So by probably about quarter past, half past 5ish, you'll be able to know exactly what we mean by no SQL.
You'll know what a graphical database is and you'll be able to import large CSVs into Neo 4J, ready for the sent element.
So, bit of a recap from Tuesday.
Big data is an ill defined term.
We've got the 5 Vs at time of recording anyway, but nobody actually agrees on what big data is.
Where is that cutoff point?
Some people say it's where we need parallel computing, some people say it's when we get to a certain level of data size.
So not very well defined.
Spoiler alert.
A lot of my courses are not going to have very well defined data terms.
You know, AI isn't a very well defined term.
Big data is not very well defined.
You'll find that non SQL is not very well defined.
It's a bit of a theme of everything I teach.
So there is this ambiguity which means we don't laugh at people when they come to you and go, I've got six USB sticks full of data.
That's big data.
It is so near SQL actually arrived by accident.
It's not a clever acronym for anything.
It was just coined by the hashtag, as we will see and they kind of ran with it.
So because of that we've got no exact definition of no SQL.
There are common characteristics that no SQL languages databases have, but no precise sticky inner definition in a dictionary.
That's the word I'm looking for and frame it.
So Big data management.
Anything that helps to support big data management is big management strategy.
It's a bit more than just sticking stuff in a database.
When it comes to big data, we have to think about who can get access to it, where are we storing it and why are we storing it so it's the organization, the administration and the security of the data.
Hence why this is a cyber module as well for young cybergloss.
And it's large volumes generally, but both structured and unstructured.
It's amazing what information we can glean from unstructured data.
So it has to be a management system that can handle large volumes of data coming in potentially very, very quickly and therefore needs to be scalable, very scalable very quickly and from different formats.
So we could be taking in video data, we could be taking in sensor data, we could be also taking insect text data.
So you'd better store that nicely as well.
Challenges we have with big data is that we don't always want everybody to have access to that data.
So we have data silos, but if we have data silos, we have duplication.
Now this place is terrible for it.
As soon as any organization gets past a certain point, there is horrible duplication.
And if there's duplication, there's own risks.
If you've got data stored in different places and they have different levels of security, slight issue there.
Especially if it's essential GDPR compliance is required.
As data storage grows, generally you're taking up more of your server space.
You're then slowing down the capacity of your network, which isn't great.
I've got a project with lots of different companies.
Actually one most biggest one is Poole Harbour.
They have given me five years of AIs data for five of their vessels.
It's approximately over 800,000 AIs entries.
I've handed that over to another partner who put it into a dashboard.
It takes somewhere between two to three minutes for any chart to load because of the sheer amount of data.
Large data, you're going to have a slower system.
Large data tends to be more complex.
You're gathering more of it, you're going to have more missing values.
People are going to be misinterpreting questions, potentially trying to match text to images presents the data provides a challenge in its own and then maintaining the quality.
If I've deployed 10 sensors across, how can.
And they're sending data back.
I know once every second can I sit there and monitor every bit of data that comes back every second from those ten sensors?
I can't.
It's ridiculous.
So being able to trust that data, how do we know that all the information coming in is true?
Deep fakes made through AI are a brilliant example of this.
If you're collecting video data, how do you know that one of the video you've got isn't a deep fake manpower.
So the number of experts you need, data analysis grows with the amount of data you have.
So that's good news for anybody who does want to go into a data analyst role, because we're all producing more data all the time, therefore lots of roles in this area.
Good news, maybe not such a challenge.
And then there are, should we affectionately call them, the dinosaurs.
So the ones that like to use paper based systems, trying to get them to convert to technological solutions is a bit of a challenge.
I had a fantastic personal example of this.
I had to have my gallbladder removed and I've got notes from my gp, I've got notes from hospital and then they sat there and there was absolutely no mention of my second child on my hospital notes whatsoever because I had him at home.
So as far as the hospital concerned, never had him, but they were looking through my notes and went, oh, you had a large dosage of anaesthetic on this date?
And I was like, yeah, I was in labour and I gave birth.
They went, oh yeah, that will do it.
That's same hospital, two different sets of paper based notes not joined up.
Yay for the NHS healthcare.
But it is scary that we have massive organisations like the NHS who are only just starting to adopt electronic healthcare records.
And even then the linkages between them is not what we would like them to be.
There are other places that are actually streets ahead of us, but if we get it right, there are a lot of benefits.
We can access data a lot quicker and a lot more structured.
So if we're doing marketing, we can tailor marketing a lot better.
Who's on social media?
Soon as you start googling something, you get the ads.
Yeah, big data.
Looking at your search history, tailoring your ads to you, hoping to make a hit.
Unfortunately for them, it's my husband that plays golf, not me.
Better customer service.
If you've got linkages between data and you can see those relationships, you can treat your customers better.
Take the NHS for example.
I was a little bit concerned that they had no knowledge of my second child or that they realized that the anesthetic I had was due to giving birth to my first child.
And that's a healthcare provider, we should be able to trust them.
And having those better database management systems can create that linkages gives you more accurate analytics.
You know, bad data, bad decision.
We can get good data stored correctly, you can get to quickly, we can make good decisions.
And then of course there's the competitive advantage which anybody in business is after.
So processing is a Form of information processing.
So we're taking those pixelated bits of information and drawing that world map and getting that information out of it.
So it is everything, collecting, storing, managing from multiple different sources.
The two main ways we do that are batch processing.
So essentially this is what you will be doing in your coursework.
You'll be batch processing where we collect a large amount of data over a particular time period and then just so send it all off to get filed wherever it needs to be done.
So this is for non time critical evaluations.
So maybe for details data, things like that.
Stream processing again as real time as real time can be.
So this is where everything just flows straight into that database.
The processing that all happens as part of the pipeline to put it in place.
So this is for those time critical kind of analysis that we need to do.
Things like an ER can make use of that.
And storing it.
Because we're storing types, we need to make sure that we can store diverse types.
So you've all had experience of SQL and relational databases.
Can you imagine trying to store an MP4 file in a relational database?
Not so easy.
Probably the closest example you might have seen is the MINIS data set.
So I know I used this for the AI course last year.
That is the pixels grayscale of the numbers that is stored as grayscale numbers in massive great big arrays.
What if we could just store it as a picture one file?
Would that not be much simpler?
If I wanted to load in 16,000 digits, I do not want to have this floor read in 16,000 tables 64 by 64 or 128 by 128 of numbers.
So we can integrate more sensors.
And there's sensors everywhere.
Nowadays everybody's got a weather sensor in their garden.
So it seems the big data storage are radical databases.
Document style databases are much better at handling data storage.
Is everybody still with me?
Excellent.
So different types in no particular order to be fair.
So distributed file systems kind of says what it says on the tin.
The information is stored across several different servers across the network.
And then multiple servers can access them, multiple users can access it.
Security kind of point of view, maybe not the ideal one, but here can do no SQL.
The ones that we're focusing on, they are the ones that say we see your tables, we see your rows and columns and we reject them, they are not useful, throw them away.
Let's get loose and fast with data and store it however the hell we want.
Data warehouses, houses for data essentially they will batch data through and store them so they Will perform the processing, the cleaning and some analytics as well depending on what sort of warehouse you go for.
There are two different ways that they will then send the data through but that's pretty what it's the transformation of the data.
It'll either take place last or in the middle.
But that's the only difference between the two different pipelines you can get there.
Or object storage, taking inspiration from object orientated programming which I'm sure you all know and love.
Maybe not.
So data is treated as a unit in this case and each one is stored as an object.
So it's got a nice architecture for handling that unstructured data.
Pros and cons well, pros are data driven decisions.
Smarter, faster decisions, more data available to us whether that is the gotten through through ill gotten means or not.
And I will show you how to collect data in it's not technically illegal, it's a gray area.
Okay, so how we collect data making informed choices.
If we can see all the data, we can see the relationships we can hopefully make best decisions.
Scalability, you don't have to worry about introducing new schemas or updating a schema and making sure everything is okay.
And efficiency but there's always a but.
Comes at a cost.
Storing it, forgetting more storage just.
Just costs money.
I mean I don't know if your students have looked and seen how much storage costs like in the cloud, but people are charging ridiculous amounts of money for is resource intensive and demandive.
You know, you only got to look at the data warehouses and the amount of carbon footprint that they have.
Which is why some people do have moral and ethical standpoints against things like ChatGPT that uses these large data warehouses.
Data security risks depending on the type of database you go for, you have to make sure everything is secure, make sure your GDPR compliant otherwise you're going to be at the foot of a rather large bill.
And then complexity.
So as we'll see from some of the graphs that we produce from probably not that complex data.
So we'll look at the IBM data set.
We've got the four movie stars, we've got runtime rating, got synopsis release year I think genre but you can create some highly complex relationships just from those six or seven categories.
And then of course you've got to be able to explain it.
Somebody needs you to jump onto a project or you're jumping on.
They're jumping onto your project.
You have to explain how everything's gone on and if it's complex they're not going to Appreciate that whatsoever.
Data privacy sometimes information privacy.
Make sure you are handling data properly.
Don't leave it on a memory stick in the middle of a train.
All those GDPR ones.
But sometimes we get a bit of a false sense of security that we're sat behind a firewall.
Must be safe.
Sat on our servers behind a firewall.
But you can use our VPN and get in that way and you have access to a whole load of information and using the data.
We will have a session a bit later on about the ethics of using big data.
One of the reasons why I don't have a smart meter is because I know exactly what I would do with that data.
So I don't want them having that data on me.
So making sure we are using it properly.
We do have GDPR which says you must be upfront and honest and tell people how you are using their data.
But do we all trust the corporations?
Yeah.
Love it.
They're all telling us the truth.
Yes.
But it is powerful.
We can tailor things better to our customers.
We can hopefully react more promptly when there are issues.
But it does come with responsibility.
With great power does come great responsibility.
We need people to trust us like you all.
Trust me.
There are the illegal implications and we want to avoid any financial repercussions.
And then of course ethical responsibility.
We shouldn't be doing anything too dodgy with the data.
Hopefully.
So although Neo4J is not the only type of non relational database that is out there, it is one of several.
It's my particular favourite because I come from a mathematical background.
So I like the fact it links to graph theory.
Hence why it's my one of choice.
But there are others out there that are widely used.
So big table Again it starts with a very sparsely populated table but then you can add to it and it will grow to billions of rows.
So you can sort of like terabytes and petabytes of data there.
The key values.
People like those because they feel more familiar.
They've got key values very similar to the relational database and they have a nice dictionary or key value that goes with them.
So you can use filters with those.
It's just more aligned to what people are used to with coming from relational databases.
I don't know if anybody's met any of those.
Some of them are paid for unfortunately.
And then document based ones look like JSON files.
Mongodb I think is the most common one that I've come across in my own research and through various student projects over the year.
So multiple different basically documents that can store different fields so you can have similar fields amongst them.
Some of them can have different ones.
It just gives you that greater flexibility, especially when we're dealing with the unstructured stuff.
So why no SQL?
So started off as a Twitter hashtag and it was chosen for a meetup.
No idea why it was chosen.
Back in 2009 and they were discussing new databases.
So it became very clear at this point that we needed something different to what had gone before.
Especially in particular they were looking at open source.
A lot of databases were hidden behind payables and you were kind of showing stuck with what they gave you.
So they wanted it to be open source for allow flexibility.
Talking about unstructured data, different formats kind of key here.
So you've got distribution and non relational.
So 2012 they decided to say it's an accidental term, no precise definition.
I don't think it still has a precise definition.
That's the wrong keyboard.
So what it's supposed to mean is not only SQL.
SQL is not the be all and end all.
So that when we are thinking about creating a database, sometimes our minds go straight.
Those tables and columns and rows, because they're familiar and they're tidy and they're ordered and it's quite nice, but doesn't always work.
So these type of databases, they're not actually new.
They have been existing since the 1960s but it took the 2000s for them to really become popular.
Think about what happened in 2000s.
The boom of the Internet and videos and YouTube kind of makes sense why they became a lot more popular.
The important thing to note is that although it's not only SQL, it still does support SQL like languages.
You've seen a little bit from Cypher with the create that it does look SQL like.
So it's not a complete change from and still does some.
I think a lot of new technologies, not just databases, do receive a lot of pushback if they're too different from anything else.
So the idea here was to make it not so different that it was unwelcoming and not adopted.
That's my completely wrong.
Although we have no definition, we have four features.
So any no SQL database will have these four features.
So it will be non relational.
It will have no fixed schema that you do not have to update when you add new data.
It doesn't have rows, it doesn't have columns, doesn't have a table at all.
It needs to be cluster friendly so it can be distributed over a network really really easily.
If you need it to be open Source flexible, be utilized by any user for any need for any purpose and that code is then made available for other people to go, oh yeah, that would be useful for mine.
Fantastic.
So much more sharing and collaborative than perhaps databases have been in the past and then 21st century web.
So that was the current state of the web.
So this is after the dot com bubble where content on the Internet started to go to be more user generated and have more user friendly interfaces rather than just being here's my website, navigate your way around it how I like you to and be done with it.
So why back with databases?
Well, there are limitations of relational databases.
You can't store certain types of data in them.
We could store text data, they're going to take up a hell of a row.
But then how would we analyze that?
How could we draw that into a query?
They also you'll find really, really slow query because you need to look up the tables and match them all across.
It does become very slow when you get large data that you're querying social networks.
They do not play nice with relational databases at all.
Some of you are on LinkedIn.
Don't worry, that fact will become known when we start doing queries.
You're sure you're on other social media networks and you follow each other, but not all of you will follow everybody else and some of you will have different interests to everybody else.
So how can we map those areas of interest?
How can we put that into a column based database and real world applications?
The one that we're going to focus on mainly is recommender systems.
When it comes to pushing content onto your timelines, Netflix, telling you what you're going to watch next.
This is where the databases so multi join queries.
Has anybody tried to do a multi join query in SQL out of interest?
Yeah.
How did you find it?
I mean it can get a little complex to formulate together, but if you've got an idea of what you're trying to achieve and how it's all mapped out, it's not so bad.
But if you're going in blind, it can be a rush trouble.
Yeah.
So graphical databases aim to try and reduce that.
If you know the relationship, you can find it a lot easier.
So schema, your schemas have to be really rigid, don't they?
With relational databases you have to know what's going into what table, how that table relates to another table, what's your primary table, what's your key?
We don't care about that.
I don't care.
We're living Loose and fast.
There are limitations in graph like queries.
So if we did want to do, you know, the multi join, where we're looking at family trees or if you're looking at supply chains, anything that you could map out as a tree in your brain, traditional is going to HP you for it and it gets slow, as I've already alluded to.
Whereas because graphical databases use tree like structures, they use a mathematical way of searching for them, which is a hell of a downside, quicker, which is nice.
So what is it?
Well, we're going to store data, not because of what the data is, but, but because how they are connected.
So we still have the information about everything.
But if I'm interested in, I'm going to pick on my sister's job.
She's a supply chain manager.
So does she really care that you know, she's getting.
Well, she does care.
She's getting so much from this company, so much from another company and they've got another subsidiary company or would she rather know that they've actually there's that chain it's got to go through and it's a change takes so long before it gets to her that impacts on her delivery for the product that she's making.
That's the bit her business we're interested in.
That time she's moaned because one of the bosses has gone, yeah, we can fill an order for a million units.
And she's gone, whoa, no, no, no, I don't have the staff, I don't have the raw materials.
They're like, oh, but I looked in the dates, like, no, you forgot lead times and wait times.
So we can see how all things work together.
So relationships between things, what works, what doesn't work.
Do I have an employer who worked for a different business, maybe a competitor, and they came to me and I'm pleased about that.
Or maybe my employees are going to my competitor, not the pleased.
We can follow how those relationships change quickly.
That's how the Twitter algorithm works.
We will look at that in a bit more detail and rather than questioning the data, we're questioning the connections.
Why is this happening?
Why is everybody going here on holiday?
Why is everybody doing this?
So connections, rather than actual data, So how they work, they follow that relationship.
Think of like little ants find food.
They're following those paths to make them a lot quicker.
They're pattern based.
So a bit more like how we think they follow that, which is quite nice.
So rather than having heavy joins, I can just go, ah, here's a relationship.
I know Exists between all the things I want to find.
Let's just query that relationship and see what it gives me or I know that, oh, these nodes have these edges.
Let's grab all those and find out.
We can picture it.
Who likes looking at really big long tables until you go cross eyed?
Yeah, me neither.
But who likes looking at pretty pictures with nice colors?
Yeah, you saw from the first session where you had those nodes with the different colors for the different tweets and the text in them.
When we get to relationships next week and you can see how they join, but it wasn't fun being able to move them around and watching them wiggle.
It's far more interactive and lets you really expand when you get the bigger ones.
When we do the movie database, please do grab the nodes, move them around, look at those relationships and see what's going on.
Expand them out because it's that picture that gives you the information.
So when to use them.
If we have complex interlinked data sets, okay, the data sets that I've given you for the Eurovision one, they're not that complex, but they are interlinked.
So one's got information about the host countries, one has information about the voting results and the other has information about the winners.
They are interlinked.
How would I do that in a relational database?
Probably very difficultly, but you'll see how you'll do it.
For the coursework we can then see when we have a deep or frequent relationship, then this is actually one of the subjects for the query that you will produce as part of the 30%.
I want you to explore voting patterns.
Who gives lots of votes to other countries, quite commonly, you know, the whole, the whole Greece, Cyprus thing, basically, I want you to find out and then when we have evolving structures in data.
So if I have a relational database and I have more data, I've got to have a new table and I've got to say that fits in the schema and I've got to update my tables.
Whereas here we can shove the nodes in and create a new relationship and anything that matches database just puts that relationship in for us.
We don't have to think about making sure those connections are there.
It does it for us, does the thinking for us.
If we do have tabular analytics, then stick with your racial databases.
You graphical databases are not the be all and end all bit like machine learning is great, but not for everything.
So again, graphical databases, not fantastic for absolutely everything.
If there are heavy aggregations, if we're pulling a lot of data together, again not quite so useful there.
And where we do have simple patterns, use those simple patterns.
Don't go for anything fancy that analogy like how do you crack open a nut?
Do you take the nutcracker or do you take the sledgehammer?
You don't generally take a sledgehammer to crack the nut.
So pick the best tool for the best job.
I'm just hopefully introducing another tool to you to add to your toolbox.
So challenging when they are distributed, scaling them becomes difficult.
Making sure everybody's got that right relationship that is all linked up is important complexity.
You can get really really stupid with your queries and you can get very lost with them.
So just be careful with that.
It's a new tool.
It's a learning curve.
You have your first introduction to it on Tuesday.
It's new.
It's scary.
Even if you've seen SQL languages before, it's a new tool in your toolbox.
Okay so there is going to be a learning curve.
Hence why I'm teaching it a little bit differently this year.
Hopefully it works.
Data ingestion and integration issues so some quite with everything sometimes doesn't always work perfectly perfectly and bringing the data into it and making sure that everything is okay doesn't always go to plan.
But where do we use it?
Social networks.
They love it.
So Twitter creates something called communities and that's how they create the two timelines in Twitter.
So we will look at how they basically go create that not the for you but the recommended one which is for the people that you don't tend to don't always follow recommendation system.
So Netflix, the scrolly bars at the bottom of ebay and Amazon.
How do they know what you're going to look at next?
Fraud and cyber security Being able to see perhaps where cyber attacks have been and see where vulnerabilities are can lead to their prevention and knowledge graphs.
How we think is useful to it has that AI linked to it.
So here is probably a really good point to pause before I go into loading.
So there's a few slides I want to talk to talk through first and then we're actually going to do them together in new four day.
So I want to show you how to load the data in and common pitfalls.
But now is probably a good time to take a break because it's quarter to five on a Thursday.
So if we say 10 minutes come back about five two is that cool And I'll check on the other room.
It.
Sam.
It.
This is the thing with regular faces it's then.
Right, so importing large data sets.
Yes, yesterday, no, Tuesday, we created single nodes at a time.
Now, as much fun as that was, we have data with more than two or three different variables.
We aren't going to want to sit there and do create, you know, a couple of hundred million times potentially.
So luckily we can make good use of our friend the CSV and load it into Neo4J.
So one of the examples I'm going to be using is see this data here?
There's a lot more of it in the CSV file.
So the two CSV files I'm going to be using can be found on the DLE in the folder wonderfully named data for in lecture examples because I am completely unimaginative and like things to do what they say on the tin.
So only five rows.
I mean, this one, I can't remember how many columns.
Five columns, probably how many rows it is.
So we need to tell it what we're doing.
So we're going to load the CSV.
We then load it with the headers.
So it's going to get that information from that first bar.
So make sure your variable names are in that first row.
We're going to tell it the file that we're going to bring it from.
We're going to tell it the function.
So there are a couple of different functions.
There is csvline with a capital l and there is CSV line with a lowercase L. I'm not entirely sure of the difference between them, but whichever one you do go with, and I can see that I've done that mistake in these slides, is make sure it's consistent.
So when we've got the create like we did on Tuesday, we're going to create the nodes, but this time we're going to give them all the same names, games.
And then this CSV line and every single one that comes after the variable names should also be a capital L. So you need to make sure it's consistent.
What I'll do is, when I'm demoing it shortly, I will deliberately make it wrong so you can see the error that will come up.
Now, if we have very, very large data sets, there is a memory limit on Neo4j to get around that we can batch truncate the data.
So we need to include in transactions of the 250 is random.
I've just picked that because it's a nice number.
You could do it in transactions of 10 rows if you wanted, or 100 rows or a thousand rows, whatever makes sense for your data.
Whichever will load in one nicely.
Again, I'll do an example of how this works with a cautionary tale.
Of course, that's the wrong screen.
The other thing we can do if you want to, is to create an index.
So indexes can be helpful when you are searching.
They can speed up that search a little bit more.
But of course with every benefit comes a price.
The price here is they do take up that additional storage space.
So you need to decide whether you have an index or not.
Cautionary tale for your coursework.
If you decide to have indexes, you must use them in your queries.
You will be docked marks in if you make an index and you don't use it.
So if you want to completely valid.
Do not worry about indexes in your coursework and just run your queries without them.
This is the safer course to go on.
Okay, I don't want to say don't use indexes.
Give them a go if you want to, but if you have them, you don't use them, you will be penalized.
Okay, consider yourselves warned.
Creating them is quite simple.
So we create an index name for a particular label name on that property.
So you're having a go at creating them within the lab sheet.
So for the Twitter data I've given you, you can create an index for the tweets and on the tweet ID and then you can search for it that way.
I'm not a huge fan of indexes, mostly because they take up space.
Space is valuable for data, so I tend to not use them.
The gain they give you in speed for the queries, sometimes it's just not worth that hassle.
But you know, play have a go with them and see how you find them.
Okay, so quick summary of the slides and then what I'm going to do is I'm going to stop sharing that screen, Wait for Neo4J to load up and then.
Come on neo.
Oh, I know it's because you were too slow.
It.
Oh excellent.
It's coming up in there.
And everyone's in green.
Fantastic.
Okay, so I can see properly.
Right.
First of all important there is getting data in there is a go to way.
I want you to do this because it will make your lives a lot easier.
You won't have to worry about file pathways at all.
So I've got a blank project here.
I'm going to create a new graphical database.
And create that one that will open up my database for me.
Make sure I've got my notes.
Well, I want to make mistakes.
I want to show you the mistakes that can easily be made.
So you can bring data in using a file path.
But I will be using this method, which is a lot easier.
And it also means that if you're working on a different machine and it's all linked up, you don't have to worry about it being in different places.
It will just be in your graphical database ready to go.
Why does this take so long?
Come on, I've waffled enough.
Should be quicker, but it's not going to.
There we go.
There we are.
So getting the data rig.
What we're going to do is if we hover the cursor over the graphical database, you can see it says start open and then three little dots.
So we're not going to start it up just yet.
What we're going to do is tap those three little dots and we're going to scroll down to open folder and we're going to open the import folder.
So that will automatically pop up with the file Explorer.
So this is the import folder directly linked to that instance of the graphical database.
I can then, no, go to the right wing and you see I've got the two CSV files there they are on the cle.
You can download them, you can scrap them and pop them straight in the downloads folder and literally just copy them into that import folder.
So they're there connected to that database.
You don't have to worry about any file paths.
They're there and I'm going to close that down and this is when I'm going to start my database.
So it's just a habit that I've got into trying putting my data into the import folder first before I start the database.
Because I have a habit of starting with database, try and import my data.
And it goes.
Don't know what you're on about.
Okay, fair enough.
So I'm going to pop that in there.
It's a bit like when you send an email with an attachment.
Always attach the attachment first and then write the email so you don't forget to attach the attachment.
Yeah, bit like my fail safe there and then we can open it.
So I'm going to first do the gaming data using just the straight load CSV and then I'm going to do the Netflix data with the transaction.
The first thing we need to tell it is let me get rid of that load CSV with headers from then the quotation marks, they will be this green color because they see there's a function at the moment.
But what will happen is I've Sort cap stuff.
It will not like that.
It will then turn brown as it realizes that it is now a string.
The colon three forward slashes, three of them, not two, not one, not four.
Three.
It's looking for the three.
And then we can just pop the name of the file in.
Remembering underscores and remembering CSV, the file extension.
Like with most programming languages, it likes you to tell you what type of file it's looking for.
If you just give it the file name, it will behave like a stubborn toddler that's hiding behind the curtain going, you can't see me, I can't see you.
Be very explicit.
Making sure it is looking for the CSV file.
Nope.
Move on.
One controlled, entered.
The nice thing about this is it like Python, it indents so you can see which parts are all part of the same.
I don't want to say query.
I'm not quite sure the word I'm looking for, but function, task.
If someone finds a better way of explaining that, please let me know.
So doing it as C S V with a capital L but lowercase I and then good old create.
Create.
Going to make absolutely everything.
Rounded brackets, colon and then just give your node, give the nodes a name that makes sense.
So here, these are all related to game releases on Steam.
So I've called the database games.
It was related to tweets, Call it tweets.
Eurovision Song Contest winners, I'm going to call it winners, I don't know, but a naming convention that makes sense to use.
And then we need the curly brackets.
So everything within this will become properties within those little circles.
First I'm going to do is title colon.
Nice thing is it predicts what I want it to do.
So I do want to have a CSV line, dot and title.
No, not tile title.
Make sure you know how your column headers look in your data.
Okay, so it is case sensitive.
So if we had title with capital T in the database, make sure that title has a capital T. And add price.
I'm only going to do the first two columns.
So what this will do is it will ignore the other three columns in the database, but just keep adding commas, giving the variable a name and then telling it that you're going to load it in with CSV and then semicolon at the end and click run or enter.
If it is successful, it will tell you.
So it's created 60 labels.
So I had 60 rows of data.
So I have 60 nodes associated with them and I have 120 properties.
I only brought in two columns of data.
So one node, two columns two times 60 is 120.
Excellent.
All my data has been loaded incorrectly.
It's a nice way of checking to make sure that all your data has been pulled in correctly.
If I wanted to run it, let's run it again.
So let's bring in the sales percentage this time.
So I'm going to call it Sales CSV line Now in the CSV file percentage has a capital P. I'm going to deliberately not do that.
So I should now have 60 labels, 60 nodes with 180 properties.
If I type that incorrectly but I run it again.
The annoying thing is it doesn't give me an error, just slightly frustrating.
But I do only still have 120 properties.
If I change that now to a capital P and run, it's still not liking me.
Of course not.
I mean nos of a lot.
See live coding.
It's great, isn't it?
Why are you not.
What have I typed wrong?
What don't you like Sales do.
Spelled.
I think I spelled something wrong, which is why it's not going through.
But you do get that nice confirmation when it does go through properly.
As I said, it's irritating that it doesn't give you that message to tell you that something hasn't gone through properly.
But if you leave dealing with large data sets, are you going to sit there and go where I've created 362, 2 million new labels and each one has got 472 properties.
You're going to sit there and want to multiply that?
Probably not.
So for small data sets, it's a nice check, but it is a bit irritating.
There isn't that kind of error that it can't find that one.
I'm hoping they might fix that in future, but not going to hold.
You'll also know if it's in, right?
Because you see I've got my little games nodes there and I can click on them and they will appear and I can move them around.
The other data I had is the Netflix.
So this one I'm going to do in transactions because it's a little bit bigger.
This is where there might be a couple of different methods.
So I'm using the desktop application which is executed actually physically on my PC.
I know some of you are using the browser Azora.
So the difference potentially is I haven't played around with the Azure version.
Is this auto in the version I have, if I don't include auto, it tells me I can't do stuff because it's part of a transaction and I'm trying to call things when I should be trying to push them or something.
It's very strange.
So in the version I've got, I need auto.
That might not be true for the versions you're using.
Okay.
If you do get errors, let me know.
Send me a picture of the screenshot and I can troubleshoot that through with you.
Try with.
Try without the main space.
Make sure the space so then it knows it's a new one.
It starts off exactly the same.
Load CSP with.
With headers ROM and we're going to do file not in caps lock this time.
1, 2, 3.
I've missed off the colon.
Netflix title CSV.
Those are using the Netflix data again.
If you don't put it in your import directory, it's going to throw it a bit of a hissy bit with you.
We still say as cse.
But now I'm going to introduce a new function called call.
So what this will do is it will truncate the data for us.
So it's calling this function and doing it in trunk transactions and curly brackets for this bit.
So anything that you want to call in the transactions goes within the curly brackets.
We have to tell it we're doing CSVLINE twice.
So again, think angry Toddler.
Needs repeating.
So with CSV line with that function, Good old create, we're going to put some more nodes in rounded brackets, colon name for the group of nodes.
The first one I'm going to do is ID csp line, dot, lowercase I and d and let's bring in the title with a capital T. Why not line, dot title.
Okay, so just bringing in those.
Those two and then after that's curly brackets, I'm going to tell it intransactions of.
Let's go 35.
That number could be anything you like.
You might need to play around with it because it will depend on the memory usage that you have.
So you could start big, work your way down and then we pray.
No.
So I didn't like it.
Couldn't learn the external file.
Is that because I can't spell exactly.
Right?
So making sure we can spell correctly is always a good one.
I meant to make things on purpose.
Good spot.
Thank you.
So here goes.
Slightly bigger data set than the other one.
I've now got 8807 labels.
With that many nodes, I've only created the two columns, so just doubled up that number.
So they're in there.
They are on the right hand side in this lovely peach pink kind of color and I can click on them and make sure they are all there.
So those are two different methods for bringing it in.
Depending on the size of your data set, depends on which one you will need to be using and depending on whether you're through the desktop execution wall like I am or through the browser one, you might need that auto, you might not.
I don't know why it's different, it just is daft.
But there we have it.
So that's kind of all the talk material for today.
There is a worksheet.
Do you have a go at loading these databases in?
I'll go around with my scanner and anybody who wants to go there you feel free because I've covered everything.
I wanted to thank you very much.
Doesn't like not having that auto.
So get rid of that and press run tells me it's implicit so that auto just forces it to do that.
That's why.
80.